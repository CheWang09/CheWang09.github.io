---
layout:     post
title:      Sequence Model (Deep Learning Ai)
author:     CHENEY WANG
tags: 		DeepLearning
header-img:  "img/manhanton.jpg"
subtitle:  	Deep learning by Andrew Ng
category:  project
---
<!-- Start Writing Below in Markdown -->

#Sequence Models
<br >
## LSTM
### Four gates:
LSTM中最重要的就是它的四个gates. 四个分别包含着记忆的丢弃和增加功能。
####Forget gate layer
 It looks at $h_{t-1}$ and $x_t$, and outputs a number between 0 and 1 for each number in the cell state $C_{t−1}$. A 1 represents “completely keep this” while a 0 represents “completely get rid of this.”
![](/img/Post_image/112691542768704_.pic_hd.jpg)
照我先前的理解是，$h_{t-1}$如果在一个句子生成模型中，将是一个单词，但是后来我发现，这不对，如果是一个单词的，present 只和上一个有关系，这样子的话，forget gate layer只存在上一个单词和现在这个单词记或者不记住这两种关系。所以$h_{t-1}$是一个vector，它保存了从第一个单词开始所生成的所有单词形成的句子。所以在forget gate layer我们可以选择去遗忘会影响我们进行下一个单词生成的那部分单词从而最终达到长记忆的作用。
#### Input gate layer
 A sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, $C̃_t$, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.
![](/img/Post_image/112661542768393_.pic_hd.jpg)

相比于之前的forget, 这个应该是update。用来更新我们在新词出现以后我们所需要更新的，其实就是说，比如，第一人称第三人称的不同会影响后面的单词生成，所以我们需要进行新的名词或者说是代词的更新。这样子的话，我的理解是，sigmoid gate所输出的应该就是按0,1分布的。也就是说和前面的forget gate一样，它所输出的形式应该是[0,1,1,0,]




