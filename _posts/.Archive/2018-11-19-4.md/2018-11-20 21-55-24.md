---
layout:     post
title:      Sequence Model (Deep Learning Ai)
author:     CHENEY WANG
tags: 		DeepLearning
header-img:  "img/manhanton.jpg"
subtitle:  	Deep learning by Andrew Ng
category:  project
---
<!-- Start Writing Below in Markdown -->

#Sequence Models
<br >
## LSTM
### Four gates:
LSTM中最重要的就是它的四个gates. 四个分别包含着记忆的丢弃和增加功能。
####Forget gate layer
 It looks at $h_{t-1}$ and $x_t$, and outputs a number between 0 and 1 for each number in the cell state $C_{t−1}$. A 1 represents “completely keep this” while a 0 represents “completely get rid of this.”
![](/img/Post_image/112691542768704_.pic_hd.jpg)
照我先前的理解是，$h_{t-1}$如果在一个句子生成模型中，将是一个单词，但是后来我发现，这不对，如果是一个单词的，present 只和上一个有关系，这样子的话，forget gate layer只存在上一个单词和现在这个单词记或者不记住这两种关系。所以$h_{t-1}$是一个vector，它保存了从第一个单词开始所生成的所有单词形成的句子。所以在forget gate layer我们可以选择去遗忘会影响我们进行下一个da部分的单词从而达到长记忆的作用。
#### Input gate layer
![](/img/Post_image/112661542768393_.pic_hd.jpg)