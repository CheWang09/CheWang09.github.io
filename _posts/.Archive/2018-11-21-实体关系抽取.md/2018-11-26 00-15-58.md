---
layout:     post
title:      实体关系抽取
author:     CHENEY WANG
tags: 		NLP Deeplearning
header-img:  "img/manhanton.jpg"
subtitle:  	文本中实体关系的抽取
category:  project
---
<!-- Start Writing Below in Markdown -->

# Building Knowledge base of recruitment post 
## Abstract
Online employment-oriented service has access to large amounts of text data, and current recommender systems always tend to collect users' operations on the website and focus on these features to satisfy the user's current need. In order to improve the accuracy of recommendations. Constructing a knowledge base through text mining from its text data plays an important role in employment-oriented service to attract users. In this report, we present a case study that builds up a knowledge base that consists of the job's name and associated workplaces, job's name and associated company name, and so on. The system first extracts entity relations from post title and user query which can help us to establish a simple knowledge base and determine what kinds of relations are useful. Then word2vec and piecewise convolutional neural network were applied to extract relation from post content. Finally, our knowledge base established not only for the recommendation system but also can be used in autonomous question answering system of our service.

## Overview of our approach
At a high level, the main purpose of our design is to improve the accuracy of recommendations. And in this report, we only discuss how to extract relations from data source and establish a domain knowledge base.
Knowledge Base Construction Instruction:
1. Collect information or data about recruitment.
2. Define the set of features to process
3. Do tokenization and label data
4. Train a classifier / extractor to use the labeled training data to not extract relations from unseen data
5. Computing the coverage of these relations.

## Data Collection
To collect data, we constructed a web crawler to visit websites of the largest online blue-collar recruitment market in China. .Below is the sample of our data set <br >
Post Content:
![](/img/Post_image/2018-11-25-20-05-38.png)

### Design
In this section, we present a solution inspired by Zeng et al. (2015) which is domain relations extracted via piecewise convolutional neural network. PCNNS are proposed for the automatic learning of features without complicated NLP preprocessing. And Figure 1 shows the neural network architecture for relation extraction. It illustrated the procedure that handles our data. This procedure includes four main parts: Vector Representation, Convolution, Piecewise Max Pooling, and Softmax Output. 
#### Vector Representation
The inputs of our model are raw word tokens. And In our method, each input word token is transformed into a vector by looking up pre-trained word embeddings. Moreover, we use position features to specify entity pairs, which are also transformed into vectors by looking up position embeddings.
#### Word Embeddings 
Word embeddings are distributed representations of words that map each word in a text to a ‘k’- dimensional real-valued vector. They have recently been shown to capture both semantic and syntactic information about words very well, setting performance records in several word similarity tasks (Mikolov et al., 2013; Pennington et al., 2014). Using word embeddings that have been trained a priori has become common practice for enhancing many other NLP tasks (Parikh et al., 2014; Huang et al., 2014). And in this project ,we used the Skip-gram model(Mikolv et al.2013) to train word embedding.  
###  Implementation
The knowledge base of 58 and Ganji platform is a NoSQL database which consists of amounts of relations of different entities. And the function of the knowledge base is to support recommendation system that knowledge base can provide user preference, user's profile. So recommendation system can use these important features to analysis each user and do the personal recommendation.<br >    So, how to extract these entities and construct these relations? Our research group has a lot of historical blue collar recruitment data which is job content post by business, user queries content and job post title. And what I did first is to tokenizer these content. Chinese is a different language from English. For example, “Jobs is the founder of Apple Inc.” which we can easily split the sentence into words by whitespace and get part of speech of each word using some library in python like NLTK. And, the sentence would be converted into the form like “Jobs[name] is the founder[noun] of Apple[organization] Inc”, in which “Jobs” is a person name and we assigned “Apple” an [organization] label as its part of speech. And commonly listed English parts of speech has noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, and sometimes numeral, article or determiner. However, in Chinese, if we have sentence like “马云是阿里巴巴创始人” which means Jack Ma is the founder of Alibaba, we would have to tokenizer this sentence into this form “马云/1[name] 是/1[v] 阿里巴巴/1[organization brand] 创始人/1[noun]” , and assign corresponding POS(part of speech) to each word. Additionally, In 58 and Ganji group, we always have a sentence like “金光小区招聘保安一名” which means Jinguang Community recruits a security. We utilized a specific domain dictionary built by 58 research group to do tokenization, and the sentence “金光小区招聘保安一名”  would be converted into this form “金光[organization brand] 小区[work place] 招聘[verb] 保安[job name] 一名[x]”.<br >
   Since we have three different data sources, we extract the entity relations from post title and user query directly based on the label we assigned to each entity. The relations extracted from these two sources is deterministic relations. In other words, the entity in the post title is always related to its requirements like workplace and job name. Similarly, for user queries, the user must search the workplace and job name which they are willing to work in and work as what they want. So at least, we can sure that above 90% relations we extracted from these two sources are deterministic relations.
<br >
    But for the content of the post, we can not deal with directly as we did in title and user queries. Because of ambiguous of entities. For instance, if a branch company want to recruit some security, they always introduce their parent company in the post, then we would extract wrong relations that parent company recruits security rather than branch company recruit securities. In this situation of ambiguous. I designed a model, an algorithm to distinguish if pair entities in a sentence have relations.
<br >
   In case of the content of the post, I used paper “Distant supervised for relation extraction without labeled data” and “Distant Supervised for relation extraction via Piecewise Convolutional Neural Networks” as a reference, and using its concept. What I did is to divide the long text into several sentences. Each sentence contains pair entities and the context between them. Therefore, the 2-dimensional matrix can be used as the input of the convolutional neural network. Most importantly, piecewise in this model is implemented in pooling level which means it does max-pooling in every three parts of the convolutional layer. Below is the picture of the piecewise convolutional neural network that it extracts the three main features of this sentence(context of entity 1, context between entity 1 and entity 2, the context of entity 2). After max-pooling, softmax is applied to do classify.

## Result and Evaluation 
## Future Work
## Conclutions





