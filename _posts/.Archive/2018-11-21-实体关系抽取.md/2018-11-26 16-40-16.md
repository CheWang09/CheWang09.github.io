---
layout:     post
title:      实体关系抽取
author:     CHENEY WANG
tags: 		NLP Deeplearning
header-img:  "img/manhanton.jpg"
subtitle:  	文本中实体关系的抽取
category:  project
---
<!-- Start Writing Below in Markdown -->

# Building Knowledge base of recruitment post 
## Abstract
Online employment-oriented service has access to large amounts of text data, and current recommender systems always tend to collect users' operations on the website and focus on these features to satisfy the user's current need. In order to improve the accuracy of recommendations. Constructing a knowledge base through text mining from its text data plays an important role in employment-oriented service to attract users. In this report, we present a case study that builds up a knowledge base that consists of the job's name and associated workplaces, job's name and associated company name, and so on. The system first extracts entity relations from post title and user query which can help us to establish a simple knowledge base and determine what kinds of relations are useful. Then word2vec and piecewise convolutional neural network were applied to extract relation from post content. Finally, our knowledge base established not only for the recommendation system but also can be used in autonomous question answering system of our service.

## Overview of our approach
At a high level, the main purpose of our design is to improve the accuracy of recommendations. And in this report, we only discuss how to extract relations from data source and establish a domain knowledge base.
Knowledge Base Construction Instruction:
1. Collect information or data about recruitment.
2. Define the set of features to process
3. Do tokenization and label data
4. Train a classifier / extractor to use the labeled training data to not extract relations from unseen data
5. Computing the coverage of these relations.

## Data Collection
To collect data, we constructed a web crawler to visit websites of the largest online blue-collar recruitment market in China. .Below is the sample of our data set <br >
Post Content:
![](/img/Post_image/2018-11-25-20-05-38.png)

## Design
In this section, we present a solution inspired by Zeng et al. (2015) which is domain relations extracted via piecewise convolutional neural network. PCNNS are proposed for the automatic learning of features without complicated NLP preprocessing. And Figure 1 shows the neural network architecture for relation extraction. It illustrated the procedure that handles our data. This procedure includes four main parts: Vector Representation, Convolution, Piecewise Max Pooling, and Softmax Output. 
![](/img/Post_image/1543209436718.jpg)
Figure 1
### Vector Representation
The inputs of our model are raw word tokens. And In our method, each input word token is transformed into a vector by looking up pre-trained word embeddings. Moreover, we use position features to specify entity pairs, which are also transformed into vectors by looking up position embeddings.
### Word Embeddings 
Word embeddings are distributed representations of words that map each word in a text to a ‘k’- dimensional real-valued vector. They have recently been shown to capture both semantic and syntactic information about words very well, setting performance records in several word similarity tasks (Mikolov et al., 2013; Pennington et al., 2014). Using word embeddings that have been trained a priori has become common practice for enhancing many other NLP tasks (Parikh et al., 2014; Huang et al., 2014). And in this project ,we used the Skip-gram model(Mikolv et al.2013) to train word embedding.  
### Position Embeddings
In relation extraction, we focus on assigning labels to entity pairs.  we use position features to specify entity pairs. A position feature is defined as the combination of the relative distances from the current word to e1 and e2. For instance, in the following example, the relative distances from "急需" to e1 ("金光小区") and e2 (“保安”) are 2 and -4, respectively. 
<p align="center"> ... 金光小区 现在 急需  招聘  小区巡逻   的   保安  一名 ... </p>
### Convolution
Convolution is one of the main building blocks of a PCNN. The term convolution refers to the mathematical combination of two functions to produce a third function. It merges two sets of information.
In the case of a PCNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map. And in this paper, convolution is executed by sliding the filter over the input which results in different feature maps. 
In the example shown in Figure 1, we assume that the length of the filter is w (w = 3); thus, $w \in \mathbb{R}^m $(m = w∗d).We consider **S** to be a sequence{q1, q2, · · · , qs}, where $q_i \in \mathbb{R}^d$.In general, let $q_{i:j}$ refer to the concatenation of $q_i$ to $q_j$. The convolution operation involves taking the dot product of w with each w-gram in the sequence **q** to obtain another sequence $c \in \mathbb{R}^{s+w−1}$:
$$
\begin{align}
c_j = wq_{j-w+1:j}
\end{align}
$$
where the index j ranges from 1 to s+w-1. Out-of-range inout values $q_i$,where i <1 or i>s, are taken to be zero.
The ability to capture different features typically requires the use of multiple filters (or feature maps) in the convolution. Under the assumption that we use n filters (W = {w1,w2,..., wn}), the convolution operation can be expressed as follows:
$$
\begin{align}
c_{ij} = w_iq_{j-w+1:j}  \ \ 1 \leq i \leq n
\end{align}
$$
The convolution result is a matrix C = {c1, c2, · · · , cn} ∈ $\mathbb{R}^{n×(s+w−1)}$. Figure 1 shows an example in which we use different filters in the convolution procedure.
### Piecewise Max Pooling
The size of the convolution output matrix $C \in \mathbb{R}^{n×(s+w−1)}$ depends on the number of tokens s in the sentence that is fed into the network. To
apply subsequent layers, the features that are extracted by the convolution layer must be combined
such that they are independent of the sentence
length. 
In traditional Convolution Neural Networks (CNNs), max pooling operations are often applied for this purpose (Collobert et al., 2011;Zeng et al., 2014).
##  Implementation
The knowledge base of 58 and Ganji platform is a NoSQL database which consists of amounts of relations of different entities. And the function of the knowledge base is to support recommendation system that knowledge base can provide user preference, user's profile. So recommendation system can use these important features to analysis each user and do the personal recommendation.<br >    So, how to extract these entities and construct these relations? Our research group has a lot of historical blue collar recruitment data which is job content post by business, user queries content and job post title. And what I did first is to tokenizer these content. Chinese is a different language from English. For example, “Jobs is the founder of Apple Inc.” which we can easily split the sentence into words by whitespace and get part of speech of each word using some library in python like NLTK. And, the sentence would be converted into the form like “Jobs[name] is the founder[noun] of Apple[organization] Inc”, in which “Jobs” is a person name and we assigned “Apple” an [organization] label as its part of speech. And commonly listed English parts of speech has noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, and sometimes numeral, article or determiner. However, in Chinese, if we have sentence like “马云是阿里巴巴创始人” which means Jack Ma is the founder of Alibaba, we would have to tokenizer this sentence into this form “马云/1[name] 是/1[v] 阿里巴巴/1[organization brand] 创始人/1[noun]” , and assign corresponding POS(part of speech) to each word. Additionally, In 58 and Ganji group, we always have a sentence like “金光小区招聘保安一名” which means Jinguang Community recruits a security. We utilized a specific domain dictionary built by 58 research group to do tokenization, and the sentence “金光小区招聘保安一名”  would be converted into this form “金光[organization brand] 小区[work place] 招聘[verb] 保安[job name] 一名[x]”.<br >
   Since we have three different data sources, we extract the entity relations from post title and user query directly based on the label we assigned to each entity. The relations extracted from these two sources is deterministic relations. In other words, the entity in the post title is always related to its requirements like workplace and job name. Similarly, for user queries, the user must search the workplace and job name which they are willing to work in and work as what they want. So at least, we can sure that above 90% relations we extracted from these two sources are deterministic relations.
<br >
    But for the content of the post, we can not deal with directly as we did in title and user queries. Because of ambiguous of entities. For instance, if a branch company want to recruit some security, they always introduce their parent company in the post, then we would extract wrong relations that parent company recruits security rather than branch company recruit securities. In this situation of ambiguous. I designed a model, an algorithm to distinguish if pair entities in a sentence have relations.
<br >
   In case of the content of the post, I used paper “Distant supervised for relation extraction without labeled data” and “Distant Supervised for relation extraction via Piecewise Convolutional Neural Networks” as a reference, and using its concept. What I did is to divide the long text into several sentences. Each sentence contains pair entities and the context between them. Therefore, the 2-dimensional matrix can be used as the input of the convolutional neural network. Most importantly, piecewise in this model is implemented in pooling level which means it does max-pooling in every three parts of the convolutional layer. Below is the picture of the piecewise convolutional neural network that it extracts the three main features of this sentence(context of entity 1, context between entity 1 and entity 2, the context of entity 2). After max-pooling, softmax is applied to do classify.

## Result and Evaluation 
## Future Work
## Conclutions





