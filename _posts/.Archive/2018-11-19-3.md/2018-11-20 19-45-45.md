---
layout:     post
title:      Support Vector Machine
author:     CHENEY WANG
tags: 		MachineLearning Review
header-img:  "img/manhanton.jpg"
subtitle:  	Support Vector Machine Review
category:  project
---
<!-- Start Writing Below in Markdown -->

# 支持向量机
<br >
## Support Vector Machine Conduction By Andrew Wu
这部分是根据吴恩达对于SVM的推导。
$$
\begin{align} 
\min_{u}C\sum_{i=1}^{m} [j^icost_i(\theta^Tx^i)+(1-y^i)cost_\theta(\theta^Tx^i)]+\frac{1}{2}\sum_{i=1}^m\theta_j^2
\end{align}
$$
In this cost function, C is used to do regularization which is similar to $\lambda$ in logistic regression.
<br>
其实，我还是没有理解，为什么我们可以直接获得这个代价函数，可能是要从原理推导太难了吧，作为ML的入门课程，他并没有从最初的状态去推导这个方程。

### Vector Inner Product (内积）
Inner product of two vectors.
$$
\begin{align}
&U =  \begin{bmatrix} 
u_1 \\ u_2 
\end{bmatrix} \\
&V = \begin{bmatrix} 
v_1 \\ v_2 
\end{bmatrix}  \ \ So \ \  U^T V = \ ? \\
&||U|| =  length \ of\  vector\ U\ that = \sqrt{u_1^2 + u_2^2} \in \mathbb{R} \\
&We\ assume\ that\ P\ is\ length\ of\ projection\ of\ v\ onto\ u \\
&U^TV = P \dot \ ||U||
\end{align}
$$

## SVM Decision Boundary.
$$
\begin{la}





