---
layout:     post
title:      Support Vector Machine
author:     CHENEY WANG
tags: 		MachineLearning Review
header-img:  "img/manhanton.jpg"
subtitle:  	Support Vector Machine Review
category:  project
---
<!-- Start Writing Below in Markdown -->

# 支持向量机
<br >
## Support Vector Machine Conduction By Andrew Wu
这部分是根据吴恩达对于SVM的推导。
$$
\begin{align} 
\min_{u}C\sum_{i=1}^{m} [j^icost_i(\theta^Tx^i)+(1-y^i)cost_\theta(\theta^Tx^i)]+\frac{1}{2}\sum_{i=1}^m\theta_j^2
\end{align}
$$
In this cost function, C is used to do regularization which is similar to $\lambda$ in logistic regression.
<br>
其实，我还是没有理解，为什么我们可以直接获得这个代价函数，可能是要从原理推导太难了吧，作为ML的入门课程，他并没有从最初的状态去推导这个方程。

### Vector Inner Product (内积）
Inner product of two vectors.
$$
\begin{align}
&U =  \begin{bmatrix} 
u_1 \\ u_2 
\end{bmatrix} \\
&V = \begin{bmatrix} 
v_1 \\ v_2 
\end{bmatrix}  \ \ So \ \  U^T V = \ ? \\
&||U|| =  length \ of\  vector\ U\ that = \sqrt{u_1^2 + u_2^2} \in \mathbb{R} \\
&We\ assume\ that\ P\ is\ length\ of\ projection\ of\ v\ onto\ u \\
&U^TV = P \dot \ ||U||
\end{align}
$$

### SVM Decision Boundary.
$$
\begin{align}
\min_\theta \frac{1}{2} \sum_{j=1}^{h} \theta_j^2 \\
S.T. \ \ \ & \theta^TX^i \geq 1 \ \ \ if\ \ y^i =1 \\
\ \ \ & \theta^TX^i \leq -1 \ \ \ if\ \ y^i =0 
\end{align}
$$

## SVM(周志华西瓜书版本)
西瓜书中从几何角度对支持向量机进行了讨论。
$$ 
\begin{equation}
\begin{cases}
    W^Tx_i + b \geq 1 \ , \ if\ y=1\\
    W^Tx_i + b \leq -1 \ , \ if\ y=-1\\
\end{cases}
\end{equation}
$$
所以异类向量（即点到平面的距离）: $r = \frac{W^Tx+b}{||W||}$。
Then, $r = \frac{2|W^Tx_i+b|} = \frac{2}{||W||}$.
Then, we have to minimize $\frac{2}{}$ 






